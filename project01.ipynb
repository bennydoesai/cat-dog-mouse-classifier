{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cat-Dog-Mouse Classifier\n",
    "\n",
    "This project is a hybrid of a multi-class classification tutorial and a binary classification project\n",
    "\n",
    "Multi-class classification tutorial: https://stackabuse.com/creating-a-neural-network-from-scratch-in-python-multi-class-classification/   \n",
    "Binary classification project: https://github.com/ardamavi/Dog-Cat-Classifier\n",
    "\n",
    "The jupyter notebook used as a template and some content was borrowed from https://www.coursera.org/learn/neural-networks-deep-learning/ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import all the packages that we will need during this project. \n",
    "- [os](https://docs.python.org/3/library/os.html) contains functions for interacting with the operating system.\n",
    "- [sys](https://docs.python.org/3/library/sys.html) provides access to system-specific parameters and functions.\n",
    "- [time](https://docs.python.org/3/library/time.html) provides various time-related functions."
    "- [keras](https://keras.io) is a deep learning library and neural networks API.\n",
    "- [numpy](https://www.numpy.org/) is the fundamental package for scientific computing with Python.\n",
    "- [h5py](http://www.h5py.org) is a common package to interact with a dataset that is stored on an H5 file.\n",
    "- [matplotlib](http://matplotlib.org) is a library to plot graphs in Python. Some parameters are defined here.\n",
    "- [scipy](https://www.scipy.org/) and [PIL](http://www.pythonware.com/products/pil/) are used here to test your model with your own picture at the end.\n",
    "- [skimage](https://scikit-image.org/) is used for image processing.\n",
    "- [sklearn](https://scikit-learn.org/) is a machine learning library.\n",
    "\n",
    "\n",
    "- *%load_ext autoreload* and *%autoreload 2* are used to auto-reload modules.\n",
    "- *np.random.seed(1)* is used to keep all the random function calls consistent.",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import keras\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from skimage import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Dataset\n",
    "\n",
    "The get_dataset() function and its get_img() helper function are used to load images from the ./Data/Train_Data/ directory and create numpy arrays X and Y in ./Data/npy_train_data/ containing the image data and one-hot-encoded labels, respectively.\n",
    "The train_test_split function from sklearn.model_selection is used to split the numpy arrays into random train and test subsets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_img(data_path):\n",
    "    # Getting image array from path:\n",
    "    img_size = 64\n",
    "    img = io.imread(data_path)\n",
    "    img = imresize(img, (img_size, img_size, 3))\n",
    "    return img\n",
    "\n",
    "def get_dataset(dataset_path='Data/Train_Data'):\n",
    "    # Getting all data from data path:\n",
    "    try:\n",
    "        X = np.load('Data/npy_train_data/X.npy')\n",
    "        Y = np.load('Data/npy_train_data/Y.npy')\n",
    "    except:\n",
    "        labels = listdir(dataset_path) # Geting labels\n",
    "        print('Categories:\n', labels)\n",
    "        len_datas = 0\n",
    "        for label in labels:\n",
    "            len_datas += len(listdir(dataset_path+'/'+label))\n",
    "        X = np.zeros((len_datas, 64, 64, 3), dtype='float64')\n",
    "        Y = np.zeros(len_datas)\n",
    "        count_data = 0\n",
    "        count_categori = [-1,''] # For encode labels\n",
    "        for label in labels:\n",
    "            datas_path = dataset_path+'/'+label\n",
    "            for data in os.listdir(datas_path):\n",
    "                img = get_img(datas_path+'/'+data)\n",
    "                X[count_data] = img\n",
    "                # For encode labels:\n",
    "                if label != count_categori[1]:\n",
    "                    count_categori[0] += 1\n",
    "                    count_categori[1] = label\n",
    "                Y[count_data] = count_categori[0]\n",
    "                count_data += 1\n",
    "        # Create dataset:\n",
    "        Y = keras.utils.to_categorical(Y)\n",
    "        if not os.path.exists('Data/npy_train_data/'):\n",
    "            os.makedirs('Data/npy_train_data/')\n",
    "        np.save('Data/npy_train_data/X.npy', X)\n",
    "        np.save('Data/npy_train_data/Y.npy', Y)\n",
    "    X /= 255.\n",
    "    X, X_test, Y, Y_test = train_test_split(X, Y, test_size=0.1, random_state=42)\n",
    "    return X, X_test, Y, Y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Run the code in this cell to import the data\n",
    "X, X_test, Y, Y_test = get_dataset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Model\n",
    "\n",
    "The get_model() function and its save_model() helper function create the model using the Keras Sequential model API (https://keras.io/models/sequential/) and save the model and weights in ./Data/Model/ as json and HDF5, respectively.\n",
    "\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    if not os.path.exists('Data/Model/'):\n",
    "        os.makedirs('Data/Model/')\n",
    "    model_json = model.to_json()\n",
    "    with open(\"Data/Model/model.json\", \"w\") as model_file:\n",
    "        model_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"Data/Model/weights.h5\")\n",
    "    print('Model and weights saved')\n",
    "    return\n",
    "\n",
    "def get_model(num_classes=3):\n",
    "    model = kera.models.Sequential()\n",
    "\n",
    "    model.add(keras.layers.Conv2D(32, (3, 3), input_shape=(64, 64, 3)))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(keras.layers.Conv2D(32, (3, 3)))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(keras.layers.Conv2D(64, (3, 3)))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(64))\n",
    "    model.add(keras.layers.Activation('relu'))\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "    model.add(keras.layers.Dense(num_classes))\n",
    "    model.add(keras.layers.Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Run the code in this cell to generate the model and save it and the weights.\n",
    "model = save_model(get_model(len(Y[0])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Train\n",
    "\n",
    "The train_model() function uses the model generated by the get_model() and save_model() functions and processes the training data and runs a validation against the test data that were input and split in the get_dataset() function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X, X_test, Y, Y_test):\n"
    "    checkpoints = []\n"
    "    if not os.path.exists('Data/Checkpoints/'):\n"
    "        os.makedirs('Data/Checkpoints/')\n"
    "    checkpoints.append(keras.callbacks.ModelCheckpoint('Data/Checkpoints/best_weights.h5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto', period=1))\n"
    "    checkpoints.append(keras.callbacks.TensorBoard(log_dir='Data/Checkpoints/./logs', histogram_freq=0, write_graph=True, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None))\n"
    "\n"
    "    # Creates live data:\n"
    "    # For better yield. The duration of the training is extended.\n"
    "\n"
    "    # If you don't want, use this:\n"
    "    # model.fit(X, Y, batch_size=10, epochs=25, validation_data=(X_test, Y_test), shuffle=True, callbacks=checkpoints)\n"
    "\n"
    "    generated_data = keras.preprocessing.image.ImageDataGenerator(featurewise_center=False, samplewise_center=False, featurewise_std_normalization=False, samplewise_std_normalization=False, zca_whitening=False, rotation_range=0,  width_shift_range=0.1, height_shift_range=0.1, horizontal_flip = True, vertical_flip = False)\n"
    "    generated_data.fit(X)\n"
    "    model.fit_generator(generated_data.flow(X, Y, batch_size=8), steps_per_epoch=X.shape[0]//8, epochs=25, validation_data=(X_test, Y_test), callbacks=checkpoints)\n"
    "\n"
    "    return model\n"
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(model, X, X_test, Y, Y_test)\n",
    "save_model(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Predict\n",
    "\n",
    "The predict() function uses the saved model's predict method to generate an output prediction for a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(model, X):\n"
    "    Y = model.predict(X)\n"
    "    Y = np.argmax(Y, axis=1)\n"
    "    if Y[0] == 0:\n"
    "        Y = 'cat' \n"
    "    elif Y[0] == 1:\n"
    "        Y = 'dog'\n"
    "    elif Y[0] == 2:\n"
    "        Y = 'mouse'\n"
    "    return Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Test with your own image\n",
    "\n",
    "You can use your own image and see the output of your model. To do that:\n",
    "    1. Add your image to this Jupyter Notebook's directory, in the \"images\" folder.\n",
    "    2. Change your image's name in the following code.\n",
    "    3. Run the code and check if the algorithm is right (0 = cat, 1 = dog, 2 = mouse)!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name = \"test_mouse.jpg\" # change this to the name of your image file \n",
    "img_dir = \"images/\" + img_name\n",
    "img = get_img(img_dir)\n",
    "X = np.zeros((1, 64, 64, 3), dtype='float64')\n",
    "X[0] = img\n",
    "# Getting model:\n",
    "model_file = open('Data/Model/model.json', 'r')\n",
    "model = model_file.read()\n",
    "model_file.close()\n",
    "model = keras.models.model_from_json(model)\n",
    "# Getting weights\n",
    "model.load_weights(\"Data/Model/weights.h5\")\n",
    "Y = predict(model, X)\n",
    "print('It is a ' + Y + ' !')\n",
    "\n"
   ]
  },
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
